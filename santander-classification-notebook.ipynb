{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar librerías útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar dataset guardado en carpeta \"data\" y explorarlo. Revisar que no hayan elementos sin valor. Se observa que es un dataset muy grande con muchas muestras (200000) y también un número relativamente alto de variables (200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_santander_data()\n",
    "\n",
    "# data.head()\n",
    "# data.info()\n",
    "# Dataset contains:\n",
    "# 1 ID column, 1 target column and 200 feature columns.\n",
    "# 200000 samples\n",
    "\n",
    "# Check for NaN values\n",
    "# nan = sum(data.isnull().sum()) # 0 NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar variables en matrix __X__ y objetivos en vector **y**. Revisar frecuencia de objetivos positivos (True). Dividir datos en sets Train/Test de forma que en todos queden muestras positivas equitativas (Stratified split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select X and y from data\n",
    "X = data.iloc[:, 2:].values\n",
    "y = data[\"target\"].values\n",
    "\n",
    "# Positive/Negative labels ratio\n",
    "# print((y == True).sum() / len(y))\n",
    "# 0.10049 -> Around 1 out of 10 samples is positive. Accuracy is not the best scoring parameter\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=8, stratify=y)\n",
    "\n",
    "# # Check whether the split has equitative positive and negative labels\n",
    "# print((y_train == True).sum() / len(y_train)) # 0.1004875\n",
    "# print((y_test == True).sum() / len(y_test)) # 0.1005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver el histograma de los valores promedios de cada variable y el histograma de las respectivas desviaciones estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of features mean values\n",
    "def hist_mean_values(X=X):\n",
    "    features_mean_values = np.mean(X, axis=0)\n",
    "    plt.figure()\n",
    "    plt.hist(features_mean_values)\n",
    "    plt.show()\n",
    "\n",
    "# hist_mean_values(X)\n",
    "\n",
    "# The features mean values has a Gaussian shape, maybe some features are correlated\n",
    "\n",
    "\n",
    "# Histogram of features std values\n",
    "def hist_std_values(X=X):\n",
    "    features_std_values = np.std(X, axis=0)\n",
    "    plt.figure()\n",
    "    plt.hist(features_std_values)\n",
    "    plt.show()\n",
    "\n",
    "# hist_std_values(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar correlación entre variables. __(¡Cödigo no funciona debido a falta de recursos del PC!)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of features\n",
    "# corr_matrix = pd.DataFrame(X_train).corr().values\n",
    "\n",
    "# Mean correlation value of features\n",
    "# mean_corr_values = np.mean(corr_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observar comportamiento de variables con valor alto de desviación estándar buscando cualquier información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 100 first values of the four least uniform features for insight purpose\n",
    "def select_highest_std_features(X):\n",
    "    features_std_values = np.std(X, axis=0)\n",
    "    features_std_values_sorted_ix = np.argsort(features_std_values)[::-1]\n",
    "    return X[:,features_std_values_sorted_ix[:4]]\n",
    "\n",
    "def plot_four_features(X_four_features):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.scatter(np.arange(100), X_four_features[:100,i])\n",
    "        if (i == 0) or (i == 2):\n",
    "            plt.ylabel('Var value')\n",
    "        if (i > 1):\n",
    "            plt.xlabel('Sample')\n",
    "    plt.show()\n",
    "\n",
    "# plot_four_features(select_highest_std_features(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar variables (StandarScaler) para mejores resultados con ciertos algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(X_train=X_train, X_test=X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    return scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir set original en nueva partición con un cuarto de los elementos para ser usada en pruebas rápidas buscando mejorar tiempo de computación. Escalar esta partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce train and test sets sample size to faster try out models\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X, y,\n",
    "                                                    test_size=0.05, train_size=0.2,\n",
    "                                                    random_state=8, stratify=y)\n",
    "\n",
    "# Check whether the split has equitative positive and negative labels\n",
    "# print((y_train_red == True).sum() / len(y_train_red)) # 0.1004875\n",
    "# print((y_test_red == True).sum() / len(y_test_red)) # 0.1005\n",
    "\n",
    "# Scale reduced X features\n",
    "X_train_scaled_red, X_test_scaled_red = scale_data(X_train_red, X_test_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar los sets usando RobustScaler por si la base de datos contiene muchos outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using RobustScaler in case dataset has many outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def scale_R_data(X_train=X_train, X_test=X_test):\n",
    "    scaler_robust = RobustScaler()\n",
    "    scaler_robust.fit(X_train)\n",
    "    return scaler_robust.transform(X_train), scaler_robust.transform(X_test)\n",
    "\n",
    "# Scale w/ robust X features\n",
    "# X_train_scaled_R, X_test_scaled_R = scale_R_data()\n",
    "\n",
    "# Scale w/ robust reduced X features\n",
    "# X_train_scaled_R_red, X_test_scaled_R_red = scale_R_data(X_train_red, X_test_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar un par de modelos para rápido montaje y pruebas. Se selecciona como primera opción el __Stochastic Gradient Descent (SGD)__ debido poco costo computacional que lo hace efectivo para grandes datasets. Como segunda opción se selecciona el __Random forest__ que si bien tarda más en ejecutarse que el SGD, está dentro de un rango de tiempo de ejecución aceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set classifiers\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=8)\n",
    "forest_clf = RandomForestClassifier(random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seleccionan como métrica para evaluar modelos: __Precision, recall y F1 score.__ Esto debido al bajo porcentaje de objetivos positivos con respecto a negativos (1/10). Se implementa función para calcular dichas métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validate precision, recall and F1 score for a given classifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def clf_scores(clf, X=X_train_scaled, y=y_train, title=None):\n",
    "    scoring = [\"precision\", \"recall\", \"f1\"]\n",
    "    scores = cross_validate(clf, X, y,\n",
    "                            cv=3, scoring=scoring)\n",
    "    train_precision = np.mean(scores[\"train_precision\"])\n",
    "    train_recall = np.mean(scores[\"train_recall\"])\n",
    "    train_f1 = np.mean(scores[\"train_f1\"])\n",
    "    test_precision = np.mean(scores[\"test_precision\"])\n",
    "    test_recall = np.mean(scores[\"test_recall\"])\n",
    "    test_f1 = np.mean(scores[\"test_f1\"])\n",
    "    if title:\n",
    "        print(title)\n",
    "    print(f\"Train Precision = {train_precision:.4f}\")\n",
    "    print(f\"Test Precision = {test_precision:.4f}\")\n",
    "    print(f\"Train Recall = {train_recall:.4f}\")\n",
    "    print(f\"Test Recall = {test_recall:.4f}\")\n",
    "    print(f\"Train F1 score = {train_f1:.4f}\")\n",
    "    print(f\"Test F1 score = {test_f1:.4f}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las métricas para los dos modelos ya mencionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for SGD classifier using standard/robust scaling\n",
    "\n",
    "# sgd_clf_scores = clf_scores(sgd_clf, title=\"SGD (not-reduced)\")\n",
    "# # SGD (not-reduced)\n",
    "# # Train Precision = 0.4187\n",
    "# # Test Precision = 0.4057\n",
    "# # Train Recall = 0.3034\n",
    "# # Test Recall = 0.2960\n",
    "# # Train F1 score = 0.3500\n",
    "# # Test F1 score = 0.3403\n",
    "# # High bias\n",
    "\n",
    "# sgd_clf_scores = clf_scores(sgd_clf, X_train_scaled_R, y_train, title=\"SGD (Robust)\")\n",
    "# # SGD (Robust)\n",
    "# # Train Precision = 0.4738\n",
    "# # Test Precision = 0.4551\n",
    "# # Train Recall = 0.2819\n",
    "# # Test Recall = 0.2770\n",
    "# # Train F1 score = 0.3507\n",
    "# # Test F1 score = 0.3419\n",
    "# # High bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for random forest classifier\n",
    "\n",
    "# forest_clf_scores = clf_scores(forest_clf, title=\"Forest (not-reduced)\")\n",
    "# # Forest (not-reduced)\n",
    "# # Train Precision = 1.0000\n",
    "# # Test Precision = 0.5542\n",
    "# # Train Recall = 0.8506\n",
    "# # Test Recall = 0.0156\n",
    "# # Train F1 score = 0.9192\n",
    "# # Test F1 score = 0.0304\n",
    "# # High variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SGD tiene high bias, random forest por el contrario tiene high variance__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que devuelve las probabilidades calculadas por los modelos para cada muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y scores (y_proba) for classifiers\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def predict_scores_sgd(sgd_clf=SGDClassifier(random_state=8), X=X_train_scaled, y_true=y_train):\n",
    "    y_scores_sgd = cross_val_predict(sgd_clf, X, y_true,\n",
    "                                     cv=3, method='decision_function')\n",
    "    return y_true, y_scores_sgd\n",
    "\n",
    "def predict_scores_forest(X=X_train_scaled, y_true=y_train,\n",
    "                          forest_clf=RandomForestClassifier(random_state=8)):\n",
    "    y_scores_forest = cross_val_predict(forest_clf, X, y_true,\n",
    "                                        cv=2, method='predict_proba')[:, 1]\n",
    "    return y_true, y_scores_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar la curva precision/recall de los modelos implementados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve function\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores, title=None):\n",
    "    plt.figure()\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    plt.plot(thresholds, precisions[:-1], 'b-', label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], 'g--', label=\"Recall\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([min(thresholds), max(thresholds)])\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Predict y_scores and plot precision-recall curve for classifiers\n",
    "\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Default SGD\")\n",
    "#\n",
    "# y_true, y_scores_forest = predict_scores_forest(X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_forest, \"Default forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabajará ahora buscando mejorar el modelo SGD. Primeramente se implementa una función que efectúa una grid search a través de dados hyper-parameters de un algoritmo clasificador. La función evalúa las métricas establecidas y los resultados se guardan en dataframes ubicados en la carpeta \"_GridSearch dataframes_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunning hyper-parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search for classifier, given a parameter grid. Save results as DataFrame pickle in given path\n",
    "# Return best estimator and DataFrame results\n",
    "def hyper_parameter_tuning(clf, param_grid, scoring, df_path, cv=3, refit_parameter=None, X=X_train_scaled, y=y_train):\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring, cv=cv,\n",
    "                               refit=refit_parameter, return_train_score=True)\n",
    "    grid_search.fit(X, y)\n",
    "    drop_columns = ['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'params',\n",
    "                    'split0_test_precision','split1_test_precision', 'std_test_precision', 'split0_train_precision',\n",
    "                    'split1_train_precision', 'std_train_precision', 'split0_test_recall', 'split1_test_recall',\n",
    "                    'std_test_recall', 'split0_train_recall', 'split1_train_recall', 'std_train_recall', 'split0_test_f1',\n",
    "                    'split1_test_f1', 'std_test_f1', 'split0_train_f1', 'split1_train_f1', 'std_train_f1']\n",
    "    results_grid_search = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_grid_search.drop(columns=drop_columns, inplace=True)\n",
    "    results_grid_search.to_csv(df_path)\n",
    "    return grid_search.best_estimator_, results_grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se harán la optimización de parámetros para el clasificador SGD buscando por separado cada una de las funciones de costo que tiene el algoritmo: Hinge, log, squared hinge, perceptron y modified huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning SGD hyper-parameters\n",
    "\n",
    "# Grid search for SGD classifier\n",
    "def grid_search_sgd(param_grid, name, cv=3, X=X_train_scaled, y=y_train):\n",
    "    sgd_clf = SGDClassifier(random_state=8, penalty='elasticnet')\n",
    "    scoring = [\"precision\", \"recall\", \"f1\"]\n",
    "    df_path = \"./GridSearch dataframes/SGD_\" + name + \".csv\"\n",
    "    return hyper_parameter_tuning(sgd_clf, param_grid, scoring, df_path, cv=cv, refit_parameter=\"precision\", X=X, y=y)\n",
    "\n",
    "\n",
    "# # Grid search for hinge loss (SVC)\n",
    "# param_grid_sgd_hinge = {\"loss\": [\"hinge\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003, 0.00001, 0.00003,\n",
    "#                                                      0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03,\n",
    "#                                                      0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                         \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_hinge = grid_search_sgd(param_grid_sgd_hinge, \"hinge\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.8\n",
    "#\n",
    "# param_grid_sgd_hinge = {\"loss\": [\"hinge\"], \"alpha\": [0.0005, 0.001, 0.002], \"l1_ratio\": [0.6, 0.8, 0.9]}\n",
    "# _, sgd_gs_results_hinge = grid_search_sgd(param_grid_sgd_hinge, \"hinge_2\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.9\n",
    "sgd_clf_hinge = SGDClassifier(random_state=8, penalty='elasticnet', alpha=0.001, l1_ratio=0.9)\n",
    "# Precision = 0.59, Recall = 0.37, F1 score = 0.46\n",
    "\n",
    "\n",
    "# # Grid search for log loss (Logistic regression)\n",
    "# param_grid_sgd_log = {\"loss\": [\"log\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003, 0.00001, 0.00003,\n",
    "#                                                  0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03,\n",
    "#                                                  0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                       \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_log = grid_search_sgd(param_grid_sgd_log, \"log\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.8\n",
    "#\n",
    "# param_grid_sgd_log = {\"loss\": [\"log\"], \"alpha\": [0.0005, 0.001, 0.002], \"l1_ratio\": [0.6, 0.8, 0.9]}\n",
    "# _, sgd_gs_results_log = grid_search_sgd(param_grid_sgd_log, \"log_2\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.9\n",
    "sgd_clf_log = SGDClassifier(random_state=8, penalty='elasticnet', alpha=0.001, l1_ratio=0.9)\n",
    "# Precision = 0.58, Recall = 0.38, F1 score = 0.46\n",
    "\n",
    "\n",
    "# # Grid search for squared hinge loss\n",
    "# param_grid_sgd_squared_hinge = {\"loss\": [\"squared_hinge\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003,\n",
    "#                                                                      0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003,\n",
    "#                                                                      0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                                 \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_squared_hinge = grid_search_sgd(param_grid_sgd_squared_hinge, \"squared_hinge\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.8\n",
    "#\n",
    "# param_grid_sgd_squared_hinge = {\"loss\": [\"squared_hinge\"], \"alpha\": [0.0005, 0.001, 0.002], \"l1_ratio\": [0.6, 0.8, 0.9]}\n",
    "# _, sgd_gs_results_squared_hinge = grid_search_sgd(param_grid_sgd_squared_hinge, \"squared_hinge_2\", 2, X_train_scaled, y_train) # Best: alpha = 0.0005, l1_ratio = 0.9\n",
    "sgd_clf_squared_hinge = SGDClassifier(random_state=8, penalty='elasticnet', alpha=0.0005, l1_ratio=0.9)\n",
    "# Precision = 0.56, Recall = 0.40, F1 score = 0.41\n",
    "\n",
    "\n",
    "# # Grid search for perceptron loss\n",
    "# param_grid_sgd_perceptron = {\"loss\": [\"perceptron\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003,\n",
    "#                                                                0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003,\n",
    "#                                                                0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                              \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_perceptron= grid_search_sgd(param_grid_sgd_perceptron, \"perceptron\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.8\n",
    "#\n",
    "# param_grid_sgd_perceptron = {\"loss\": [\"perceptron\"], \"alpha\": [0.0005, 0.001, 0.002], \"l1_ratio\": [0.6, 0.8, 0.9]}\n",
    "# _, sgd_gs_results_perceptron = grid_search_sgd(param_grid_sgd_perceptron, \"perceptron_2\", 2, X_train_scaled, y_train) # Best: alpha = 0.0005, l1_ratio = 0.9\n",
    "sgd_clf_perceptron = SGDClassifier(random_state=8, penalty='elasticnet', alpha=0.0005, l1_ratio=0.9)\n",
    "# Precision = 0.56, Recall = 0.38, F1 score = 0.46\n",
    "\n",
    "\n",
    "# # Grid search for modified_huber loss\n",
    "# param_grid_sgd_modified_huber = {\"loss\": [\"modified_huber\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003,\n",
    "#                                                                        0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003,\n",
    "#                                                                        0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                                  \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_modified_huber= grid_search_sgd(param_grid_sgd_modified_huber, \"modified_huber\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.8\n",
    "#\n",
    "# param_grid_sgd_modified_huber = {\"loss\": [\"modified_huber\"], \"alpha\": [0.0005, 0.001, 0.002], \"l1_ratio\": [0.6, 0.8, 0.9]}\n",
    "# _, sgd_gs_results_modified_huber = grid_search_sgd(param_grid_sgd_modified_huber, \"modified_huber_2\", 2, X_train_scaled, y_train) # Best: alpha = 0.001, l1_ratio = 0.9\n",
    "sgd_clf_modified_huber = SGDClassifier(random_state=8, penalty='elasticnet', alpha=0.0001, l1_ratio=0.9)\n",
    "# Precision = 0.58, Recall = 0.39, F1 score = 0.46\n",
    "\n",
    "\n",
    "# Tuned SGD classifiers improved from default SGD around: Precision: 42%, Recall: 24%, F1 score: 32%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son similares para cada uno de los modelos, el porcentaje de mejora con respecto al SGD default es aproximadamente:\n",
    "\n",
    "__Precision: 42%, Recall: 24%, F1 score: 32%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar curva precision/recall de estos 5 nuevos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict reduced y_scores and plot precision-recall curve for SGD classifiers\n",
    "\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(sgd_clf_hinge, X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Hinge\")\n",
    "#\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(sgd_clf_log, X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Log\")\n",
    "#\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(sgd_clf_squared_hinge, X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Squared hinge\")\n",
    "#\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(sgd_clf_perceptron, X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Perceptron\")\n",
    "#\n",
    "# y_true, y_scores_sgd = predict_scores_sgd(sgd_clf_modified_huber, X_train_scaled, y_train)\n",
    "# plot_precision_recall_curve(y_true, y_scores_sgd, \"Modified huber\")\n",
    "\n",
    "# Very similar behaviour. Not good precision/recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar mejoras en modelo SGD usando RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid search for hinge loss (SVC) with robust scaling\n",
    "# param_grid_sgd_hinge_robust = {\"loss\": [\"hinge\"], \"alpha\": [0.0000001, 0.0000003, 0.000001, 0.000003, 0.00001, 0.00003,\n",
    "#                                                      0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03,\n",
    "#                                                      0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000],\n",
    "#                         \"l1_ratio\": [0.2, 0.8]}\n",
    "# _, sgd_gs_results_hinge_robust = grid_search_sgd(param_grid_sgd_hinge_robust, \"hinge_robust\", 2, X_train_scaled_R, y_train) # Best: alpha = 0.000003, l1_ratio = 0.8\n",
    "# Worse than standard scaling. Precision = 0.38, Recall = 0.31, F1 score = 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar curva de aprendizaje del modelo SGD tuneado con respecto al número de epochs para ver si este está convergiendo o si es necesario ajustar los parámetros de asociados al __learning rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epochs learning curves of SGD classifier to see if it converges\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def plot_SGD_epochs_learning_curve(max_epochs=1000, max_iter_sgd=100):\n",
    "    if max_iter_sgd > max_epochs:\n",
    "        print(\"max_epochs must be greater or equal to max_iter_sgd\")\n",
    "    else:\n",
    "        # Split reduced data into train and validation sets\n",
    "        X_train_learn, X_val_learn, y_train_learn, y_val_learn = train_test_split(X_train_scaled_red, y_train_red,\n",
    "                                                                                  test_size=0.2, random_state=8,\n",
    "                                                                                  stratify=y_train_red)\n",
    "\n",
    "        # Classifier with fixed max_iter to check F1 score over epochs\n",
    "        sgd_clf_step = SGDClassifier(max_iter=max_iter_sgd, random_state=8, penalty='elasticnet', loss=\"hinge\",\n",
    "                                     alpha=0.001, l1_ratio=0.9, warm_start=True, tol=-np.infty)\n",
    "\n",
    "        # Create arrays of F1 scores over epochs\n",
    "        n_epochs = int(max_epochs/max_iter_sgd)\n",
    "        train_scores_array = np.empty(n_epochs)\n",
    "        val_scores_array = np.empty(n_epochs)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # print(int((epoch+1) * max_iter_sgd))\n",
    "            # random_ix = np.random.permutation(len(y_train_learn))\n",
    "            # sgd_clf_step.fit(X_train_learn[random_ix,:], y_train_learn.ravel()[random_ix])\n",
    "            sgd_clf_step.fit(X_train_learn, y_train_learn.ravel())\n",
    "            y_train_learn_predict = sgd_clf_step.predict(X_train_learn)\n",
    "            y_val_learn_predict = sgd_clf_step.predict(X_val_learn)\n",
    "            train_score = f1_score(y_train_learn, y_train_learn_predict)\n",
    "            val_score = f1_score(y_val_learn, y_val_learn_predict)\n",
    "            train_scores_array[epoch] = train_score\n",
    "            val_scores_array[epoch] = val_score\n",
    "\n",
    "        # Plot train and validation learning curves\n",
    "        plt.figure()\n",
    "        plt.plot(val_scores_array, \"b-\", linewidth=3, label=\"Validation set\")\n",
    "        plt.plot(train_scores_array, \"r--\", linewidth=2, label=\"Training set\")\n",
    "        plt.legend(loc=\"upper right\", fontsize=14)\n",
    "        plt.xlabel((\"Epoch x\" + str(max_iter_sgd)), fontsize=14)\n",
    "        plt.ylabel(\"F1 score\", fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "# plot_SGD_learning_curve(100, 10) # After around 40 epochs, the SGD classifier converges. No need to tune learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de 40 epochs, el algoritmo converge, no es necesario modificar parámetros asociados al learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar curva de aprendizaje de modelo para observar comportamiento con respecto al número de muestras evaluadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class learning curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(clf, X, y, train_sizes_n=10, scoring=\"f1\", name=None):\n",
    "    train_sizes = np.linspace(.1, 1.0, train_sizes_n)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(clf, X, y, cv=2, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Train score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"b\", label=\"Validation score\")\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring)\n",
    "    if name:\n",
    "        plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# SGD hinge learning curve for F1 score\n",
    "# plot_learning_curve(sgd_clf_hinge, X_train_scaled_red, y_train_red, train_sizes_n=10, name=\"Reduced set - SGD hinge\") # High bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posible solución al high bias del modelo SGD es aumentar las variables expandiéndolas polinómicamente. Sin embargo, debido al gran tamaño del dataset, se ha de proceder con cuidado, expandir las 200 variables a dos dimensiones resultaría en un total de 20301 variables, valor demasiado grande para ser viable.\n",
    "\n",
    "Posible solución es combinar entonces con PCA de tal modo que primero se reduzcan las variables y entonces sí efectuar la expansión polinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High bias possible solution: Expand features with polynomial ones. Memory problem due sample size, oprion PCA reduction\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Expand X features with PolynomialFeatures\n",
    "def poly_expand(X_train=X_train_red, X_test=X_test_red, degree=2, scale=True):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    if scale:\n",
    "        X_train_poly, X_test_poly = scale_data(X_train_poly, X_test_poly)\n",
    "    return X_train_poly, X_test_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Returns X_train and X_test with n_components features after PCA\n",
    "def pca_X(X_train=X_train, X_test=X_test,\n",
    "          n_components=150, whiten=False, scale=True):\n",
    "    pca = PCA(n_components, whiten=whiten)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    if scale:\n",
    "        X_train_pca, X_test_pca = scale_data(X_train_pca, X_test_pca)\n",
    "    return X_train_pca, X_test_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar primero métricas al usar PCA para ver comportamiento. Combinar entonces PCA con expansión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test SGD classifiers without whiten\n",
    "# X_train_pca, X_test_pca = pca_X()\n",
    "# clf_scores(sgd_clf, X_train_pca, y_train, \"SGD with PCA\") # Worse recall\n",
    "# clf_scores(sgd_clf_hinge, X_train_pca, y_train, \"Tuned SGD with PCA (150 components)\") # Worse recall\n",
    "#\n",
    "#\n",
    "# # Test SGD classifiers with whiten\n",
    "# X_train_pca, X_test_pca = pca_X(whiten=True)\n",
    "# clf_scores(sgd_clf, X_train_pca, y_train, \"SGD with PCA (Whiten)\") # Worse precision, slight improvement the other against no whiten\n",
    "# clf_scores(sgd_clf_hinge, X_train_pca, y_train, \"Tuned SGD with PCA (Whiten)\") # Worse precision, slight improvement the other against no whiten\n",
    "#\n",
    "#\n",
    "# # Test SGD classifiers with whiten and more components\n",
    "# X_train_pca, X_test_pca = pca_X(whiten=True, n_components=150)\n",
    "# X_train_pca_scaled, _ = scale_data(X_train_pca, X_test_pca)\n",
    "# clf_scores(sgd_clf_hinge, X_train_pca, y_train, \"Tuned SGD with PCA (Whiten) and 150 components\") # Almost equal to Tuned SGD w/ robust scaling\n",
    "#\n",
    "#\n",
    "# # Test SGD classifiers with whiten and most components\n",
    "# X_train_pca, X_test_pca = pca_X(whiten=True, n_components=190)\n",
    "# X_train_pca_scaled, _ = scale_data(X_train_pca, X_test_pca)\n",
    "# clf_scores(sgd_clf_hinge, X_train_pca, y_train, \"Tuned SGD with PCA (Whiten) and 190 components\") # Almost equal to Tuned SGD w/ robust scaling\n",
    "# # Tuned SGD with PCA (Whiten) and 190 components\n",
    "# # Train Precision = 0.5264\n",
    "# # Test Precision = 0.5280\n",
    "# # Train Recall = 0.4131\n",
    "# # Test Recall = 0.4135\n",
    "# # Train F1 score = 0.4627\n",
    "# # Test F1 score = 0.4636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine PCA with Polynomial features. Test on reduced sets\n",
    "# X_train_pca, X_test_pca = pca_X(X_train_red, X_test_red, n_components=50, whiten=True)\n",
    "# X_train_poly, X_test_poly = poly_expand(X_train_pca, X_test_pca)\n",
    "\n",
    "# clf_scores(sgd_clf, X_train_poly, y_train_red, \"PCA - Poly SGD\")\n",
    "# clf_scores(sgd_clf_hinge, X_train_poly, y_train_red, \"PCA - Poly SGD hinge\") # Promising but memory problem with big sample size data\n",
    "# PCA - Poly SGD hinge\n",
    "# Train Precision = 0.4787\n",
    "# Test Precision = 0.2670\n",
    "# Train Recall = 0.1308\n",
    "# Test Recall = 0.0813\n",
    "# Train F1 score = 0.2054\n",
    "# Test F1 score = 0.1247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es promisorio pero en efecto es demasiado complejo para computar con el dataset completo. Se descarta la idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar comportamiento de clasificador Random Forest usando la reducción de dimensionalidad (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test forest classifiers with whiten and most components\n",
    "# X_train_pca, X_test_pca = pca_X(whiten=True, n_components=190)\n",
    "# X_train_pca_scaled, _ = scale_data(X_train_pca, X_test_pca)\n",
    "# clf_scores(forest_clf, X_train_pca, y_train, \"Forest with PCA (Whiten) and 190 components\") # Recall = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posible solución para mejorar el rendimiento de SGD es expandir el espacio de las variables usando la aproximación de kernel de sklearn que permite a este algoritmo efectuar algo similar al kernel trick de las Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel approximation\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "\n",
    "def rbf_map(X_train=X_train_red, X_test=X_test_red, gamma=0.2,\n",
    "           rbfsampler=True, n_components=100, scale=False):\n",
    "    if rbfsampler:\n",
    "        feature_map = RBFSampler(gamma=gamma, random_state=8,\n",
    "                                 n_components=n_components)\n",
    "    else:\n",
    "        feature_map = Nystroem(gamma=gamma, random_state=8,\n",
    "                               n_components=n_components)\n",
    "    X_train_mapped = feature_map.fit_transform(X_train)\n",
    "    X_test_mapped = feature_map.transform(X_test)\n",
    "    if scale:\n",
    "        X_train_mapped, X_test_mapped = scale_data(X_train_mapped, X_test_mapped)\n",
    "    return X_train_mapped, X_test_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar SGD usando nuevo espacio de 600 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RBF kernel map of X and scale it\n",
    "X_train_rbf, X_test_rbf = rbf_map(X_train_red, X_test_red, n_components=600,\n",
    "                                  rbfsampler=True, scale=True, gamma=100)\n",
    "\n",
    "\n",
    "# hist_mean_values(X_train_rbf)\n",
    "\n",
    "# Test SGD classifier with RBF mapped features\n",
    "# clf_scores(sgd_clf, X_train_rbf, y_train_red, \"RBF kernel and default SGD\") # High variance\n",
    "# RBF kernel and default SGD\n",
    "# Train Precision = 0.1553\n",
    "# Test Precision = 0.0989\n",
    "# Train Recall = 0.1350\n",
    "# Test Recall = 0.0876\n",
    "# Train F1 score = 0.1443\n",
    "# Test F1 score = 0.0929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba de nuevo reducir las variables para luego expandirlas usando la aproximación de kernel RBF. Esto debido a que posiblemente muchas de las variables son reduntantes o están correlacionadas y reducirlas podría mejorar el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine PCA with RBF kernel approximation. Test on reduced sets\n",
    "# X_train_pca, X_test_pca = pca_X(X_train_red, X_test_red, n_components=50, whiten=True, scale=False)\n",
    "# X_train_pca_rbf, X_test_pca_rbf = rbf_map(X_train_pca, X_test_pca, n_components=600, scale=True, gamma=10)\n",
    "#\n",
    "# clf_scores(sgd_clf, X_train_pca_rbf, y_train_red, \"PCA - RBF SGD\")\n",
    "# # PCA - RBF SGD\n",
    "# # Train Precision = 0.1356\n",
    "# # Test Precision = 0.0967\n",
    "# # Train Recall = 0.1279\n",
    "# # Test Recall = 0.0905\n",
    "# # Train F1 score = 0.1315\n",
    "# # Test F1 score = 0.0933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se logra mejorar el modelo SGD. Se selecciona como mejor modelo el SGD con función de costo \"Hinge\" que equivale al __SVC__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a hacer una mejoramiento de hyper-parameters de clasificador Random forest, la búsqueda es mucho más limitada que la anterior debido al mayor tiempo computacional requerido para este modelo. Se busca principalmente regular el modelo de tal modo que se reduzca el high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning forest hyper-parameters\n",
    "\n",
    "# Plot random forest learning curves\n",
    "# plot_learning_curve(forest_clf, X_train_scaled, y_train, train_sizes_n=5, name=\"Reduced set - Random forest\", scoring=\"precision\"), # High variance\n",
    "\n",
    "\n",
    "# Grid search function for Random forest classifier\n",
    "def grid_search_forest(param_grid, name, cv=2, X=X_train_scaled_red, y=y_train_red):\n",
    "    forest_clf = RandomForestClassifier(random_state=8)\n",
    "    scoring = [\"precision\", \"recall\", \"f1\"]\n",
    "    df_path = \"./GridSearch dataframes/forest_\" + name + \".csv\"\n",
    "    return hyper_parameter_tuning(forest_clf, param_grid, scoring, df_path, cv=cv, refit_parameter=\"precision\", X=X, y=y)\n",
    "\n",
    "\n",
    "# # Grid search for n_estimators\n",
    "# param_grid_forest_trees = {\"n_estimators\": [3, 10, 30, 100]}\n",
    "# _, gs_results_forest_trees = grid_search_forest(param_grid_forest_trees, \"trees\", 2, X_train_scaled_red, y_train_red) # Best: n_estimators 3, 10\n",
    "#\n",
    "# param_grid_forest_trees = {\"n_estimators\": [5, 7]}\n",
    "# _, gs_results_forest_trees = grid_search_forest(param_grid_forest_trees, \"trees_2\", 2, X_train_scaled_red, y_train_red) # Best:\n",
    "#\n",
    "# # Grid search for max_depth\n",
    "# param_grid_forest_max_depth = {\"max_depth\": [3, 10, 30, None]}\n",
    "# _, gs_results_forest_max_depth = grid_search_forest(param_grid_forest_max_depth, \"max_depth\", 2, X_train_scaled_red, y_train_red) # Best: max_depth 30\n",
    "#\n",
    "# param_grid_forest_max_depth = {\"max_depth\": [15, 60, 70, 80]} # Best: 40, 50\n",
    "# _, gs_results_forest_max_depth = grid_search_forest(param_grid_forest_max_depth, \"max_depth_3\", 2, X_train_scaled_red, y_train_red) # Best: max_depth 30\n",
    "#\n",
    "# # Grid search for max_features\n",
    "# param_grid_forest_max_features = {\"max_features\": [5, 15, 50, 100, 200]}\n",
    "# _, gs_results_forest_max_features = grid_search_forest(param_grid_forest_max_features, \"max_features\", 2, X_train_scaled_red, y_train_red) # Too much memory\n",
    "\n",
    "# Grid search for max_leaf_nodes\n",
    "# param_grid_forest_max_leaf_nodes = {\"max_leaf_nodes\": [125, 140, 160, 180, 210]}\n",
    "# _, gs_results_forest_max_leaf_nodes = grid_search_forest(param_grid_forest_max_leaf_nodes, \"max_leaf_nodes_2\", 2, X_train_red, y_train_red) # Best: 150 0.75 Precision - 210 0.77 Precision\n",
    "#\n",
    "# # Grid search for\n",
    "# param_grid_forest_ = {\"\": []}\n",
    "# _, gs_results_forest_ = grid_search_forest(param_grid_forest_, \"\", 2, X_train_scaled_red, y_train_red) # Best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest_clf_tuned = RandomForestClassifier(random_state=8, n_estimators=5, max_depth=35, max_leaf_nodes=210)\n",
    "# clf_scores(forest_clf_tuned, X_train_red, y_train_red, title=\"Tuned forest\")\n",
    "# # Tuned forest\n",
    "# # Train Precision = 0.9916\n",
    "# # Test Precision = 0.4644\n",
    "# # Train Recall = 0.1031\n",
    "# # Test Recall = 0.0077\n",
    "# # Train F1 score = 0.1868\n",
    "# # Test F1 score = 0.0152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba la reducción de dimensionalidad PCA buscando eliminar el high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduction and testing with forest\n",
    "# X_train_pca_1, X_test_pca_1 = pca_X(X_train_red, X_test_red, n_components=150, whiten=False, scale=False)\n",
    "#\n",
    "# X_train_pca_2, X_test_pca_2 = pca_X(X_train_red, X_test_red, n_components=150, whiten=True, scale=False)\n",
    "#\n",
    "# X_train_pca_3, X_test_pca_3 = pca_X(X_train_red, X_test_red, n_components=100, whiten=False, scale=False)\n",
    "#\n",
    "# X_train_pca_4, X_test_pca_4 = pca_X(X_train_red, X_test_red, n_components=100, whiten=True, scale=False)\n",
    "#\n",
    "#\n",
    "# clf_scores(forest_clf, X_train_pca_1, y_train_red, title=\"Random forest with pca_1\") # Test Precision = 0.4520\n",
    "# clf_scores(forest_clf, X_train_pca_2, y_train_red, title=\"Random forest with pca_2\") # Test Precision = 0.4651\n",
    "# clf_scores(forest_clf, X_train_pca_3, y_train_red, title=\"Random forest with pca_3\") # The other two very bad for the 4 cases\n",
    "# clf_scores(forest_clf, X_train_pca_4, y_train_red, title=\"Random forest with pca_4\")\n",
    "# Random forest with pca_2\n",
    "# Train Precision = 0.9999\n",
    "# Test Precision = 0.4651\n",
    "# Train Recall = 0.8447\n",
    "# Test Recall = 0.0070\n",
    "# Train F1 score = 0.9157\n",
    "# Test F1 score = 0.0137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se logra eliminar el high variance del modelo Random forest por lo que se procede a calcular las métricas designadas en el test set para evaluar los resultados finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classifiers on X_train and X_test\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Scores of classifier in train and test sets\n",
    "def test_scoring(clf, X_train=X_train_scaled, X_test=X_test_scaled,\n",
    "                 y_train=y_train, y_test=y_test, name=None):\n",
    "    if name:\n",
    "        print(name)\n",
    "\n",
    "    clf_test = clone(clf)\n",
    "    clf_test.fit(X_train, y_train)\n",
    "    # clf_test.fit(X_train_scaled, y_train.ravel())\n",
    "\n",
    "    y_train_predict = clf_test.predict(X_train)\n",
    "    y_test_predict = clf_test.predict(X_test)\n",
    "\n",
    "    train_precision = precision_score(y_train, y_train_predict)\n",
    "    test_precision = precision_score(y_test, y_test_predict)\n",
    "    print(\"Train Precision: \", train_precision)\n",
    "    print(\"Test Precision: \", test_precision)\n",
    "\n",
    "    train_recall = recall_score(y_train, y_train_predict)\n",
    "    test_recall = recall_score(y_test, y_test_predict)\n",
    "    print(\"Train Recall: \", train_recall)\n",
    "    print(\"Test Recall: \", test_recall)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_predict)\n",
    "    test_f1 = f1_score(y_test, y_test_predict)\n",
    "    print(\"Train F1: \", train_f1)\n",
    "    print(\"Test F1: \", test_f1)\n",
    "\n",
    "    return [test_precision, test_recall, test_f1, train_precision, train_recall, train_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar resultados de métricas, en train/test sets, para el modelo SGD que fue el de mejor rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of train and test scores\n",
    "def scores_bar(scores, title=None):\n",
    "    labels = [\"Precision\", \"Recall\", \"F1 score\"]\n",
    "    train_scores = scores[3:]\n",
    "    test_scores = scores[:3]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width / 2, test_scores, width, label='Test')\n",
    "    rects2 = ax.bar(x + width / 2, train_scores, width, label='Train')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    plt.ylim([0, 1])\n",
    "    # fig.tight_layout()\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        ax.set_title('Scores')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# sgd_test_scores = test_scoring(sgd_clf_hinge, X_train_scaled, X_test_scaled, y_train, y_test, \"Tuned SGD\")\n",
    "# scores_bar(sgd_test_scores, \"SGD\")\n",
    "# Tuned SGD\n",
    "# Train Precision:  0.6017008504252126\n",
    "# Test Precision:  0.588871096877502\n",
    "# Train Recall:  0.37405149894265455\n",
    "# Test Recall:  0.36592039800995024\n",
    "# Train F1:  0.4613201396080236\n",
    "# Test F1:  0.451365449524394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver matriz de confusión de los dos clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confussion matrix of classifiers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def conf_matrix(clf, X_train=X_train_scaled, X_test=X_test_scaled,\n",
    "                 y_train=y_train, y_test=y_test):\n",
    "    clf_test = clone(clf)\n",
    "    clf_test.fit(X_train, y_train)\n",
    "    # clf_test.fit(X_train_scaled, y_train.ravel())\n",
    "\n",
    "    y_train_predict = clf_test.predict(X_train)\n",
    "    y_test_predict = clf_test.predict(X_test)\n",
    "\n",
    "    test_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "    train_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "    return test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(conf_matrix(sgd_clf_hinge))\n",
    "# [[34953  1027]\n",
    "#  [ 2549  1471]]\n",
    "# Recall is very low, a lot of positive transactions are not predicted (2549 from a total of 4020)\n",
    "# depending on what should be more important, the precision/recall tradeoff could be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se soluciona problema de high bias\n",
    "\n",
    "Para modelo SGD, recall es muy bajo, muchas transacciones efectuadas (positivas) no son predecidas (2549 de un total de 4020)\n",
    "\n",
    "Dependiendo de qué es más importante para el banco, se podría ajustar el precision/recall tradeoff, sin embargo el modelo general no será nunca bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(conf_matrix(forest_clf))\n",
    "# [[35908    72]\n",
    "#  [ 3935    85]]\n",
    "\n",
    "\n",
    "# High bias of SGD not solved. Nonetheless it performs as a classifier\n",
    "# High variance of forest not solved. Model can not be said to be a classifier due its terrible performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se soluciona problema de high variance.\n",
    "\n",
    "Modelo tiene un rendimiento demasiado pobre para ser implementado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
